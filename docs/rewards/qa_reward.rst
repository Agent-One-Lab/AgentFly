.. _qa_reward:

Question Answering Rewards
===========================

.. currentmodule:: agents.agents.rewards.qa_reward

QA reward functions evaluate agent performance on question answering tasks using F1 score and exact match metrics.

qa_f1_reward
------------

.. autofunction:: qa_f1_reward

**Function Signature:**

.. code-block:: python

    def qa_f1_reward(prediction: str, golden_answer: str, trajectory: List[str]) -> dict

**Description:** Evaluates QA performance using F1 score and exact match metrics.

**Parameters:**
    - **prediction** (str): Agent's predicted answer
    - **golden_answer** (str): Correct answer
    - **trajectory** (List[str]): Agent's conversation trajectory (unused in basic version)

**Returns:**
    dict: Dictionary containing:
        - **reward** (float): F1 score (0.0 to 1.0)
        - **f1** (float): F1 score between prediction and golden answer
        - **em** (float): Exact match score (0.0 or 1.0)
        - **precision** (float): Precision score
        - **recall** (float): Recall score

qa_f1_reward_format
-------------------

.. autofunction:: qa_f1_reward_format

**Function Signature:**

.. code-block:: python

    def qa_f1_reward_format(prediction: str, answer: str, trajectory: List[Dict]) -> dict

**Description:** F1-based QA evaluation with tool usage requirement.

**Parameters:**
    - **prediction** (str): Agent's predicted answer
    - **answer** (str): Correct answer
    - **trajectory** (List[Dict]): Agent's conversation trajectory

**Returns:**
    dict: Dictionary containing:
        - **reward** (float): F1 score if tool used (â‰¥2 tool calls), 0.0 otherwise
        - **f1** (float): F1 score between prediction and answer
        - **em** (float): Exact match score
        - **precision** (float): Precision score
        - **recall** (float): Recall score

Technical Details
-----------------

**Text Normalization:**
    - Removes articles (a, an, the)
    - Normalizes whitespace
    - Removes punctuation
    - Converts to lowercase

**F1 Score Calculation:**
    - Token-based overlap between prediction and ground truth
    - Precision = common_tokens / prediction_tokens  
    - Recall = common_tokens / ground_truth_tokens
    - F1 = 2 * (precision * recall) / (precision + recall)

**Exact Match (EM):**
    - Binary score: 1.0 if normalized answers are identical, 0.0 otherwise
    - Special handling for yes/no/noanswer responses

**Tool Usage Detection:**
    - Counts messages with "tool" role in trajectory
    - Requires minimum 2 tool calls (including final answer)
    - Returns 0.0 reward if insufficient tool usage

**Example Usage:**

.. code-block:: python

    # Basic F1 evaluation
    result = qa_f1_reward("Paris is the capital", "Paris", [])
    print(result)
    # {"reward": 0.67, "f1": 0.67, "em": 0.0, "precision": 0.5, "recall": 1.0}
    
    # With tool usage requirement
    trajectory = [
        {"role": "assistant", "content": "I need to search for information"},
        {"role": "tool", "content": "search results"},
        {"role": "assistant", "content": "Based on my search, the answer is Paris"}
    ]
    result = qa_f1_reward_format("Paris", "Paris", trajectory)
    print(result)
    # {"reward": 1.0, "f1": 1.0, "em": 1.0, "precision": 1.0, "recall": 1.0}

**Special Cases:**
    - Yes/No questions: Must match exactly or return 0.0
    - Empty predictions: Return 0.0 for all metrics
    - No token overlap: Return 0.0 for all metrics

**Use Cases:**
    - Reading comprehension evaluation
    - Information retrieval task assessment
    - Knowledge-based question answering
    - Tool-augmented QA system evaluation

**Metric Interpretation:**
    - **F1 Score**: Harmonic mean of precision and recall, good overall metric
    - **Exact Match**: Strict evaluation requiring perfect answers
    - **Precision**: Relevance of prediction tokens
    - **Recall**: Coverage of ground truth tokens 